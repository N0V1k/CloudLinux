{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# итоговый код\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from collections import Counter\n",
    "import codecs\n",
    "from glob import glob\n",
    "import pickle\n",
    "import hashlib \n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Регулярное выражение для извлечения слов из кода\n",
    "split_re = re.compile(r\"([^\\w\\d\\-])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция создания словаря вероятностей появления символа, нужна для того, \n",
    "# чтобы в последующем считать энтропию символов строки\n",
    "# На вход она получает путь к файлам, а возвращает словарь,\n",
    "# где символы - это ключи, а вероятность встречи - значение по ключу\n",
    "def Create_Dictionary(files):\n",
    "    charprobabilitydict = {}\n",
    "    files = Path(files)\n",
    "    for path_object in files.glob('**/*.php'): # Пробегаемся по всем php файлам во всех подпапках заданного пути\n",
    "        if path_object.is_file():\n",
    "            with codecs.open(path_object, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                text = f.read()\n",
    "                a = Counter(text)\n",
    "                for key in a:\n",
    "                    if (key in charprobabilitydict) != 1:\n",
    "                        charprobabilitydict[key] = a[key] / len(text) \n",
    "                    else:\n",
    "                        charprobabilitydict[key] = (charprobabilitydict[key] + a[key] / len(text)) / 2\n",
    "            \n",
    "    return charprobabilitydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция нужна для подсчёта вероятностей символов в новых файлах и\n",
    "# добавления их в уже существующий словарь\n",
    "# На вход принимает существующий словарь и файл, из которого\n",
    "# мы считываем вероятности символов\n",
    "# Возвращает она изменённый словарь\n",
    "def Dictionary_update(charprobabilitydict, file):\n",
    "    with codecs.open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            text = f.read()\n",
    "            a = Counter(text)\n",
    "            for key in a:\n",
    "                if (key in charprobabilitydict) != 1:\n",
    "                    charprobabilitydict[key] = a[key] / len(text) \n",
    "                else:\n",
    "                    charprobabilitydict[key] = (charprobabilitydict[key] + a[key] / len(text)) / 2\n",
    "    \n",
    "    return charprobabilitydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция нарезки строк из наших файлов, принимает на вход строку и словарь вероятностей,\n",
    "# нарезает строку на слова, после этого считает посимвольную энтропию строки\n",
    "# Возвращает список, где первый элемент в списке - это энтропия строки,\n",
    "# а все после последующие элементы - слова строки\n",
    "def spliter(code, charprobablitydict):\n",
    "    code = str(code).strip(r\"\\\\r\\\\n\").strip(r\"\\r\\n\").strip(\"\\n\").strip(\"\\\\n\")\n",
    "    for i in range(10):\n",
    "        sum = 0\n",
    "        for j in range(len(code)):\n",
    "            sum += - np.sum(float(charprobablitydict[code[j]]) * np.log2(float(charprobablitydict[code[j]]))) # считает энтропию\n",
    "    return [sum] + [w for w in split_re.split(code) if w and w != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция создания матрицы из строк файла, \n",
    "# на вход принимает php файл, словарь вероятностей и кол-во слов в строке(выбирали сами)\n",
    "# После чего из строк собирает матрицу \n",
    "# Выводит матрицу из всех строк файла нарезанные функцией spliter\n",
    "def Entropy_Matrix(file, charprobabilitydict, number_of_characters):\n",
    "    with codecs.open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        matrix_entropy = []\n",
    "        for line in f.readlines():\n",
    "            parsed = spliter(line[0:number_of_characters], charprobabilitydict)\n",
    "            if parsed and parsed[0] != 0:\n",
    "                matrix_entropy.append(parsed)\n",
    "    \n",
    "    return matrix_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция выбора более важных для нас строк, мы добавили опасные функции, которые расположены в определённом порядке,\n",
    "# от самых опасных к менее опасным. \n",
    "# Сначала мы добавляем в файл все строки с опасными функциями(но не больше 20), после этого добавляем строки\n",
    "# со средним значением энтропии(кол-во строк регулирует показатель len_str), после этого добавляем такое же кол-во строк\n",
    "# с минимальным и максимальным значением энтропии, после этого дозаполняем матрицу строк до 40 штук рандомными значениями\n",
    "# На вход функция получает матрицу из всех строк файла и кол-во строк каждой категории, возвращает новую матрицу из 40 строк\n",
    "def Slicing(entropy_matrix, len_str):\n",
    "    new_matrix = []\n",
    "    entropy_matrix = np.sort(entropy_matrix, axis=0)\n",
    "    average_value = 0\n",
    "    # Список опасных функций\n",
    "    danger_functions = ['pcntl_exec', 'proc_open', 'passthru', 'shell_exec', 'popen', 'exec', 'system', 'create_function', \\\n",
    "                        'assert', 'preg_replace', 'eval', 'include', 'include_once', 'require', 'require_once', 'array_walk', \\\n",
    "                        'array_map', 'phpinfo', 'mail', 'header', 'fopen', 'tmpfile', 'copy', 'extract', 'file_put_contents', \\\n",
    "                        'move_uploaded_file', 'symlink', 'touch', 'file_get_contents', 'chmod', 'ob_start', 'assert_options', \\\n",
    "                        'register_shutdown_function', 'register_tick_function', 'uksort', 'usort', 'preg_replace_callback', \\\n",
    "                        'iterator_apply', 'call_user_func', 'call_user_func_array', 'get_current_user']\n",
    "    index = []\n",
    "#     Если файл менее 40 строк, то добавляем все\n",
    "    if len(entropy_matrix) <= 40:\n",
    "        new_matrix = entropy_matrix\n",
    "    else:\n",
    "#     Сначала добавляем строки содержащие опасные функции    \n",
    "        for i in range(len(entropy_matrix)):\n",
    "            if len(index) > 20:\n",
    "                break\n",
    "            for func in danger_functions:\n",
    "                if func in entropy_matrix[i]:\n",
    "                    new_matrix.append(entropy_matrix[i])\n",
    "                    index.append(i)\n",
    "                break\n",
    "        for i in index:\n",
    "            entropy_matrix = np.delete(entropy_matrix, i)\n",
    "\n",
    "#     Теперь добавляем строки со средним значением\n",
    "        for i in range(len(entropy_matrix)):\n",
    "            average_value += entropy_matrix[i][0]\n",
    "        average_value = average_value / len(entropy_matrix)\n",
    "      \n",
    "        for i in range(len_str):\n",
    "            for i in range(len(entropy_matrix) - 1):\n",
    "                if (entropy_matrix[i][0] <= average_value) and (entropy_matrix[i + 1][0] >= average_value):\n",
    "                    new_matrix.append(entropy_matrix[i])\n",
    "                    entropy_matrix = np.delete(entropy_matrix, i)\n",
    "                    \n",
    "#   Добавляем строки с максимальным и минимальным значением\n",
    "        for i in range(len_str):\n",
    "            new_matrix.append(entropy_matrix[i])\n",
    "            entropy_matrix = np.delete(entropy_matrix, i)\n",
    "            new_matrix.append(entropy_matrix[len(entropy_matrix) - i - 1])\n",
    "            entropy_matrix = np.delete(entropy_matrix, len(entropy_matrix) - i - 1)\n",
    "#   Оставшиеся строки докидываем рандомно(до 40 во сём файле)\n",
    "        for i in range(25 - len(index)):\n",
    "            k = random.randint(0,len(entropy_matrix) - 1)\n",
    "            new_matrix.append(entropy_matrix[k])\n",
    "            entropy_matrix = np.delete(entropy_matrix, k)\n",
    "\n",
    "    return new_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эта функция добавляет к нашей матрице файла ещё 1 строку, которая содержит в первой ячейке - категорию файла,\n",
    "# во-второй ячейке - энтропию пути и названия файла, а в третьей - путь и название файла, разбитые на слова\n",
    "# На вход получает нашу отредактированную матрицу из 40 строк файла, сам Path файла и словарь вероятностей(для подсчёта энтропии)\n",
    "# На выходе выдаёт матрицу из 41 строки файла\n",
    "def Path_matrix(new_matrix, file, dictionary):\n",
    "    path_matrix = []\n",
    "    file_stats = os.stat(file) # Посчитали вес файла\n",
    "    first = file_stats.st_size / 1000\n",
    "    # Распределяем файл по категориям по размеру на диске, всего есть 41 категория\n",
    "    # распределение происходит так, если файл весит меньше 1 Кб - то это 0 категория, если меньше 2Кб - то это 2 и т.д.\n",
    "    category = 0\n",
    "    if first <= 1:\n",
    "        category = 0\n",
    "    elif first <= 10:\n",
    "        category = round(first)\n",
    "    elif first <= 100:\n",
    "        category = round(first / 10) + 10\n",
    "    elif first <= 1000:\n",
    "        category = round(first / 100) + 20\n",
    "    elif first <= 10000:\n",
    "        category = round(first / 1000) + 30\n",
    "    else:\n",
    "        category = 41\n",
    "# Добавляем строку с категорией файла в первой ячейке, а дальше нарезаем её, как делали раньше с помощью функции spliter\n",
    "    path_matrix.append([category] + spliter(str(file).split(\"\\\\\", 5)[-1], dictionary))\n",
    "    for i in new_matrix:\n",
    "        path_matrix.append(i)\n",
    "    return path_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция нужна для того, чтобы привязать нашу матрицу файла к самому файлу, делается это с помощью словаря,\n",
    "# где ключ - это hash файла, а значение по ключу - матрица файла\n",
    "# На вход функция получает сам файл, матрицу файла и словарь, куда будет записан сам файл с его матрицей\n",
    "# Выводит функция словарь dict[hash] = matrix\n",
    "def Dict_hash(file, entropy_matrix, dict_hash):\n",
    "    with open(file,\"rb\") as f:\n",
    "        bytes = f.read() \n",
    "        readable_hash = hashlib.sha256(bytes).hexdigest();\n",
    "        dict_hash[readable_hash] = entropy_matrix\n",
    "    \n",
    "    return dict_hash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция записывает словарь с файлами в отдельный файл\n",
    "# На вход получает сам словарь и файл, в который она его запишет\n",
    "def Import_file(dict_hash, file):\n",
    "    \n",
    "    with open(file, 'wb') as f:\n",
    "        pickle.dump(dict_hash, f)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция считывает содержимое файла, на вход получает файл, откуда нужно считать информацию,\n",
    "# а выводит всё содержимое файла в переменную\n",
    "def Export_file(dictionary_file):\n",
    "    \n",
    "    with open(dictionary_file,'rb') as f:\n",
    "        der = pickle.load(f)\n",
    "\n",
    "    return der "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_Create_Dictionary (__main__.TestNotebook) ... ok\n",
      "test_Dict_hash (__main__.TestNotebook) ... C:\\Users\\rtyde\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n",
      "ok\n",
      "test_Dictionary_update (__main__.TestNotebook) ... ok\n",
      "test_Entropy_Matrix (__main__.TestNotebook) ... ok\n",
      "test_Export_file (__main__.TestNotebook) ... ok\n",
      "test_Import_file (__main__.TestNotebook) ... ok\n",
      "test_Path_matrix (__main__.TestNotebook) ... ok\n",
      "test_Slicing (__main__.TestNotebook) ... ok\n",
      "test_spliter (__main__.TestNotebook) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 9 tests in 7.461s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x189fcf45190>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "# Unitets\n",
    "\n",
    "class TestNotebook(unittest.TestCase):\n",
    "    \n",
    "    def test_Create_Dictionary(self):\n",
    "        self.assertIsNotNone(Create_Dictionary(r'C:\\Users\\rtyde\\Cloud\\CloudLinux\\safe'))\n",
    "        self.assertDictEqual(Create_Dictionary(r'C:\\Users\\rtyde\\Cloud\\CloudLinux\\safe'), Create_Dictionary(r'C:\\Users\\rtyde\\Cloud\\CloudLinux\\safe'))\n",
    "        self.assertTrue(type(Create_Dictionary(r'C:\\Users\\rtyde\\Cloud\\CloudLinux\\safe')) is dict)\n",
    "        self.assertTrue(type(Create_Dictionary(r'C:\\Users\\rtyde\\Cloud\\CloudLinux\\safe')['!']) is float)\n",
    "    \n",
    "    \n",
    "    def test_Dictionary_update(self):\n",
    "        dictionary = Create_Dictionary(r'C:\\Users\\rtyde\\Cloud\\CloudLinux\\safe')\n",
    "        new_dictionary = Dictionary_update(dictionary, r'C:\\Users\\rtyde\\Cloud\\CloudLinux\\wordpress-5.8\\wordpress\\index.php')\n",
    "        \n",
    "        self.assertIsNotNone(new_dictionary)\n",
    "        self.assertFalse(self.assertDictEqual(dictionary, new_dictionary))\n",
    "        self.assertTrue(type(new_dictionary) is dict)\n",
    "        self.assertNotEqual(dictionary[';'], Dictionary_update(dictionary, r'C:\\Users\\rtyde\\Cloud\\CloudLinux\\wordpress-5.8\\wordpress\\wp-load.php')[';'])\n",
    "\n",
    "\n",
    "    def test_spliter(self):\n",
    "        dictionary = Create_Dictionary(r'C:\\Users\\rtyde\\Cloud\\CloudLinux')\n",
    "        split = spliter(r'C:\\Users\\rtyde\\Cloud\\CloudLinux\\wordpress-5.8\\wordpress\\wp-load.php', dictionary)\n",
    "    \n",
    "        self.assertIsNotNone(split)\n",
    "        self.assertTrue(type(split) is list)\n",
    "        self.assertTrue(type(split[0]) is np.float64)\n",
    "    \n",
    "    \n",
    "    def test_Entropy_Matrix(self):\n",
    "        dictionary = Create_Dictionary(r'C:\\Users\\rtyde\\Cloud\\CloudLinux')\n",
    "        entropy_matrix = Entropy_Matrix(r'C:\\Users\\rtyde\\Cloud\\CloudLinux\\wordpress-5.8\\wordpress\\wp-load.php', dictionary, 250)\n",
    "        \n",
    "        self.assertIsNotNone(entropy_matrix)\n",
    "        for i in entropy_matrix:\n",
    "            self.assertTrue(type(i[0]) is np.float64)\n",
    "            self.assertTrue(type(i[1]) is str)\n",
    "        self.assertTrue(type(entropy_matrix) is list)\n",
    "        \n",
    "    \n",
    "    def test_Slicing(self):\n",
    "        file= r'C:\\Users\\rtyde\\Cloud\\CloudLinux\\wordpress-5.8\\wordpress\\wp-load.php'\n",
    "        dictionary = Create_Dictionary(r'C:\\Users\\rtyde\\Cloud\\CloudLinux')\n",
    "        entropy_matrix = Entropy_Matrix(file, dictionary, 250)\n",
    "        slic = Slicing(entropy_matrix, 5)\n",
    "        f = open(file, 'r')\n",
    "        len_count = sum(1 for line in f)\n",
    "        \n",
    "        self.assertIsNotNone(slic)\n",
    "        if  len_count >= 40:\n",
    "            self.assertEqual(len(slic), 40)\n",
    "        else:\n",
    "            self.assertEqual(len(slic), len_count)\n",
    "        f.close()\n",
    "        self.assertTrue(type(slic) is list)\n",
    "        \n",
    "        \n",
    "    def test_Path_matrix(self):\n",
    "        file= r'C:\\Users\\rtyde\\Cloud\\CloudLinux\\wordpress-5.8\\wordpress\\wp-load.php'\n",
    "        dictionary = Create_Dictionary(r'C:\\Users\\rtyde\\Cloud\\CloudLinux')\n",
    "        entropy_matrix = Entropy_Matrix(file, dictionary, 250)\n",
    "        slic = Slicing(entropy_matrix, 5)\n",
    "        path_matrix = Path_matrix(slic, file, dictionary)\n",
    "        \n",
    "        self.assertIsNotNone(path_matrix)\n",
    "        self.assertTrue(type(path_matrix[0][0]) is int)\n",
    "        self.assertTrue(type(path_matrix[0][1]) is np.float64)\n",
    "        self.assertTrue(type(path_matrix[0][2]) is str)\n",
    "        self.assertEqual(len(slic) + 1, len(path_matrix))\n",
    "        \n",
    "        \n",
    "    def test_Dict_hash(self):\n",
    "        file= r'C:\\Users\\rtyde\\Cloud\\CloudLinux\\wordpress-5.8\\wordpress\\wp-load.php'\n",
    "        dictionary = Create_Dictionary(r'C:\\Users\\rtyde\\Cloud\\CloudLinux')\n",
    "        entropy_matrix = Entropy_Matrix(file, dictionary, 250)\n",
    "        slic = Slicing(entropy_matrix, 5)\n",
    "        path_matrix = Path_matrix(slic, file, dictionary)\n",
    "        dict_hash = {}\n",
    "        res_dict = Dict_hash(file, path_matrix, dict_hash)\n",
    "        \n",
    "        self.assertIsNotNone(res_dict)\n",
    "        self.assertTrue(type(res_dict) is dict)\n",
    "        self.assertTrue(type(res_dict[list(res_dict.keys())[0]]) is list)\n",
    "        self.assertEqual(len(res_dict[list(res_dict.keys())[0]]), len(path_matrix))\n",
    "        \n",
    "        \n",
    "    def test_Import_file(self):\n",
    "        file= r'C:\\Users\\rtyde\\Cloud\\CloudLinux\\wordpress-5.8\\wordpress\\wp-load.php'\n",
    "        dictionary = Create_Dictionary(r'C:\\Users\\rtyde\\Cloud\\CloudLinux')\n",
    "        entropy_matrix = Entropy_Matrix(file, dictionary, 250)\n",
    "        slic = Slicing(entropy_matrix, 5)\n",
    "        path_matrix = Path_matrix(slic, file, dictionary)\n",
    "        dict_hash = {}\n",
    "        res_dict = Dict_hash(file, path_matrix, dict_hash)\n",
    "        dictionary_file = Import_file(res_dict, 'dict_hash.txt')\n",
    "        \n",
    "        with open('dict_hash.txt','rb') as f:\n",
    "            der = pickle.load(f)\n",
    "            self.assertIsNotNone(der)\n",
    "            self.assertDictEqual(der, res_dict)\n",
    "            self.assertTrue(type(der[list(der.keys())[0]]) is list)\n",
    "            \n",
    "            \n",
    "    def test_Export_file(self):\n",
    "        file= r'C:\\Users\\rtyde\\Cloud\\CloudLinux\\wordpress-5.8\\wordpress\\wp-load.php'\n",
    "        dictionary = Create_Dictionary(r'C:\\Users\\rtyde\\Cloud\\CloudLinux')\n",
    "        entropy_matrix = Entropy_Matrix(file, dictionary, 250)\n",
    "        slic = Slicing(entropy_matrix, 5)\n",
    "        path_matrix = Path_matrix(slic, file, dictionary)\n",
    "        dict_hash = {}\n",
    "        res_dict = Dict_hash(file, path_matrix, dict_hash)\n",
    "        dictionary_file = Import_file(res_dict, 'dict_hash.txt')\n",
    "        res = Export_file('dict_hash.txt')\n",
    "        \n",
    "        self.assertIsNotNone(res)\n",
    "        self.assertTrue(type(res) is dict)\n",
    "        self.assertDictEqual(res, res_dict)\n",
    "    \n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dictionares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание словаря вероятностей \n",
    "files = Path(r'C:\\Users\\rtyde\\Cloud\\CloudLinux')\n",
    "dictionary = Create_Dictionary(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавление в словарь вероятностей файлов разных типов(SA, SAFE,INJ,UNK)\n",
    "files = Path(r'C:\\Users\\rtyde\\Cloud\\CloudLinux\\Test')\n",
    "for path_object in files.glob('**/'+h):\n",
    "    if path_object.is_file():\n",
    "        dictionary = Dictionary_update(dictionary, path_object)\n",
    "Import_file(dictionary, 'dictionary.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем словарь вероятностей в файл\n",
    "dictionary = Export_file('dictionary.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание словаря SA файлов(файлы вордпресса) и импорт словаря в файл\n",
    "files = Path(r'C:\\Users\\rtyde\\Cloud\\CloudLinux\\Test\\SA_phpfiles')\n",
    "h = '?'*64 # Это маска, потому что не у всех файлов есть расширение\n",
    "dict_hash = {}\n",
    "for path_object in files.glob('**/'+h):\n",
    "    if path_object.is_file():\n",
    "        print(path_object)\n",
    "        matr = Path_matrix(Slicing(Entropy_Matrix(path_object, dictionary, 250), 5), path_object, dictionary)\n",
    "        dict_hash = Dict_hash(path_object, matr, dict_hash)\n",
    "Import_file(dict_hash, 'SA_Test.txt')        \n",
    "print(dict_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание словаря INJ файлов и импорт словаря в файл\n",
    "files = Path(r'C:\\Users\\rtyde\\Cloud\\CloudLinux\\Test\\INJ_phpfiles')\n",
    "h = '?'*64\n",
    "dict_hash = {}\n",
    "for path_object in files.glob('**/'+h):\n",
    "    if path_object.is_file():\n",
    "        print(path_object)\n",
    "        matr = Path_matrix(Slicing(Entropy_Matrix(path_object, dictionary, 250), 5), path_object, dictionary)\n",
    "        dict_hash = Dict_hash(path_object, matr, dict_hash)\n",
    "Import_file(dict_hash, 'INJ_Test.txt') \n",
    "print(dict_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание словаря SAFE файлов(это файлы и гугл диска) и запись словаря в файл\n",
    "files = Path(r'C:\\Users\\rtyde\\Cloud\\CloudLinux\\Test\\SAFE_phpfiles')\n",
    "h = '?'*64\n",
    "dict_hash = {}\n",
    "for path_object in files.glob('**/'+h):\n",
    "    if path_object.is_file():\n",
    "        print(path_object)\n",
    "        matr = Path_matrix(Slicing(Entropy_Matrix(path_object, dictionary, 250), 5), path_object, dictionary)\n",
    "        dict_hash = Dict_hash(path_object, matr, dict_hash)\n",
    "Import_file(dict_hash, 'SAFE_Test.txt') \n",
    "print(dict_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание словаря UNK файлов и импорт его в файл\n",
    "files = Path(r'C:\\Users\\rtyde\\Cloud\\CloudLinux\\Test\\UNK_phpfiles')\n",
    "h = '?'*64\n",
    "dict_hash = {}\n",
    "for path_object in files.glob('**/'+h):\n",
    "    if path_object.is_file():\n",
    "        print(path_object)\n",
    "        matr = Path_matrix(Slicing(Entropy_Matrix(path_object, dictionary, 250), 5), path_object, dictionary)\n",
    "        dict_hash = Dict_hash(path_object, matr, dict_hash)\n",
    "Import_file(dict_hash, 'UNK_Test.txt') \n",
    "print(dict_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание словаря SAFE файлов WordPress-a и запись его в файл\n",
    "files = Path(r'C:\\Users\\rtyde\\Cloud\\CloudLinux')\n",
    "dict_hash = {}\n",
    "for path_object in files.glob('**/*.php'):\n",
    "    if path_object.is_file():\n",
    "        print(path_object)\n",
    "        matr = Path_matrix(Slicing(Entropy_Matrix(path_object, dictionary, 250), 5), path_object, dictionary)\n",
    "        dict_hash = Dict_hash(path_object, matr, dict_hash)\n",
    "Import_file(dict_hash, 'WordPress.txt')        \n",
    "print(dict_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выгружаем словари\n",
    "dict_hash = Export_file('dict_hash(old).txt')\n",
    "SA = Export_file('SA_Test.txt')\n",
    "SAFE = Export_file('SAFE_Test.txt')\n",
    "UNK = Export_file('UNK_Test.txt')\n",
    "INJ = Export_file('INJ_Test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для статистического взвешивания элементов матрицы с помощью TF-IDF для словарей файлов, она создаёт новый словарь, где на вход подаётся наш словарь файлов.\n",
    "# На выходе функция создаёт новый словарь, ключ нового словаря - hash файла,\n",
    "# а значение - это матрица из словарей, где по ключу мы вызываем значение из матрицы нашего прошлого словаря.\n",
    "# Вывод примерно такой:\n",
    "# print(dict_Tf_IDF['00731ed48b28b30f4583466f631489519325c77c0bc68597c2e00add00236248'])\n",
    "#\n",
    "#[{1: 0.1858614279519973, 4.123289312938014: 0.1858614279519973, 'safe': 0.1858614279519973, '\\\\': 0.11770124870633125,\n",
    "#  'safe_0395775733f263f35c5d5d0fe1c9c2748066ff09b556558d530f5657b9cb07f4': 0.1858614279519973, '.': 0.05684857266743394,\n",
    "#  'php': 0.11770124870633125}, {0.022447084697324208: 0.6505149978319906, '}': 0.5}, {0.09590458196907431: 0.4336766652213271,\n",
    "#  '<': 0.20068666377598746, '?': 0.27463624698143957}, {0.1141567729236648: 0.4336766652213271, '?': 0.27463624698143957,\n",
    "#  '>': 0.20068666377598746}, {1.5678506668856307: 0.6505149978319906, '}': 0.5}, {2.486148620051607: 0.10841916630533177,\n",
    "#  'if': 0.10841916630533177, '(': 0.03799432963747703, '$': 0.01848739580136303, '_POST': 0.043573228773361464,\n",
    "#  '[': 0.043573228773361464, \"'\": 0.06632333477867293, 'name': 0.050171665943996864, ']': 0.043573228773361464,\n",
    "#  ')': 0.03799432963747703, '{': 0.08333333333333333}]\n",
    "\n",
    "def TF_IDF(dict_hash):\n",
    "    res_dict = {}\n",
    "\n",
    "    def compute_tf(text): # Подсчёт TF\n",
    "        tf_text = Counter(text)\n",
    "        for i in tf_text:\n",
    "            tf_text[i] = tf_text[i]/float(len(text))\n",
    "        return tf_text\n",
    "    \n",
    "    def compute_idf(word, dict_hash): # Подсчёт IDF\n",
    "        return math.log10(len(dict_hash)/sum([1.0 for i in dict_hash if word in i]))\n",
    "    \n",
    "\n",
    "    for i in dict_hash.keys():\n",
    "        documents_list = []\n",
    "        for text in dict_hash[i]:\n",
    "            tf_idf_dictionary = {}\n",
    "            computed_tf = compute_tf(text)\n",
    "            for word in computed_tf:\n",
    "                tf_idf_dictionary[word] = computed_tf[word] * compute_idf(word, dict_hash[i])\n",
    "            documents_list.append(tf_idf_dictionary)\n",
    "        res_dict[i] = documents_list\n",
    "            \n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for creating result dictionary with TF-IDF tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция преобразует из двух словарей: исходного и полученного с TF-IDF векторизацией в один словарь\n",
    "# Это нужно, чтобы сохранить значения энтропии и категории файла, и не заменить их на их значения TF-IDF\n",
    "# На вход подаются два словаря с файлами - исходный и словарь с TF-IDF\n",
    "# Функция возвращает итоговый словарь\n",
    "def Merge_dict(dict_hash, new_dict): \n",
    "    res_dict = {}\n",
    "\n",
    "    for i in dict_hash.keys():\n",
    "        matr = []\n",
    "        for j in range(len(dict_hash[i])):\n",
    "            matrix = []\n",
    "            for k in range(len(dict_hash[i][j])):\n",
    "                if j == 0:\n",
    "                    if k == 0 or k == 1:\n",
    "                        matrix.append(dict_hash[i][j][k])\n",
    "                    else:\n",
    "                        matrix.append(new_dict[i][j][dict_hash[i][j][k]])\n",
    "                else:\n",
    "                    if k == 0:\n",
    "                        matrix.append(dict_hash[i][j][k])\n",
    "                    else:\n",
    "                        matrix.append(new_dict[i][j][dict_hash[i][j][k]])\n",
    "            matr.append(matrix) \n",
    "            res_dict[i] = matr\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём словари TF-IDF\n",
    "new_dict = TF_IDF(dict_hash)\n",
    "SA_dict = TF_IDF(SA)\n",
    "SAFE_dict = TF_IDF(SAFE)\n",
    "UNK_dict = TF_IDF(UNK)\n",
    "INJ_dict = TF_IDF(INJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём итоговые словари\n",
    "res_dict = Merge_dict(dict_hash, new_dict)\n",
    "res_SA = Merge_dict(SA, SA_dict)\n",
    "res_SAFE = Merge_dict(SAFE, SAFE_dict)\n",
    "res_UNK = Merge_dict(UNK, UNK_dict)\n",
    "res_INJ = Merge_dict(INJ, INJ_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Записываем словари в файлы\n",
    "Import_file(res_dict, 'result_dictionary.txt')\n",
    "Import_file(res_SA, 'SA_dict.txt')\n",
    "Import_file(res_SAFE, 'SAFE_dict.txt')\n",
    "Import_file(res_UNK, 'UNK_dict.txt')\n",
    "Import_file(res_INJ, 'INJ_dict.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция создаёт датасет, где hash это лейбл, а матрицу файла разворачиваем и создаём одну длинную строку 41*255\n",
    "# На вход функция получает файл со словарём, а возвращает DataFrame\n",
    "def Create_Dataset(file):\n",
    "    dict_hash = Export_file(file)\n",
    "    df = pd.DataFrame(columns=np.arange(255*41)) \n",
    "    for i in dict_hash.keys():\n",
    "        res = []\n",
    "        for line in dict_hash[i]:\n",
    "            string = np.ones(255) # создаём строку из 255 единиц\n",
    "            for j in range(len(line)):\n",
    "                string[j] = line[j] # перезаписываем строку нашего файла туда\n",
    "            res.extend(string) # добавляем каждую строку в конец следующей\n",
    "        s = pd.Series(res, name = i)\n",
    "        df = df.append(s)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём датасеты для каждой категории файлов\n",
    "df_res = Create_Dataset('result_dictionary.txt')\n",
    "df_sa = Create_Dataset('SA_dict.txt')\n",
    "df_safe = Create_Dataset('SAFE_dict.txt')\n",
    "df_inj = Create_Dataset('INJ_dict.txt')\n",
    "df_ink = Create_Dataset('UNK_dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переименовываем столбец\n",
    "df_res.rename(columns={'Unnamed: 0': 'Hash'}, inplace=True)\n",
    "df_sa.rename(columns={'Unnamed: 0': 'Hash'}, inplace=True)\n",
    "df_safe.rename(columns={'Unnamed: 0': 'Hash'}, inplace=True)\n",
    "df_inj.rename(columns={'Unnamed: 0': 'Hash'}, inplace=True)\n",
    "df_unk.rename(columns={'Unnamed: 0': 'Hash'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Создаём столбец категории файла и размечаем файлы\n",
    "df_res['Category'] = [1 for i in range(len(data_word))]\n",
    "df_sa['Category'] = [2 for i in range(len(data_SA))]\n",
    "df_safe['Category'] = [1 for i in range(len(data_SAFE))]\n",
    "df_inj['Category'] = [3 for i in range(len(data_INJ))]\n",
    "df_unk['Category'] = [4 for i in range(len(data_UNK))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединяем все датасеты в один\n",
    "data = pd.concat([df_res, df_sa, df_safe, df_inj, df_unk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заполняем пропущенные значения \n",
    "data = data.fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция округления списка\n",
    "def rounding(vect):\n",
    "    res = []\n",
    "    for i in vect:\n",
    "        res.append(round(i))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция поиска среднего значения в строке\n",
    "# На вход подаётся список, а выход - это среднее значение в списке\n",
    "def Mean_list(dict_hash):\n",
    "    mean = []\n",
    "    for line in dict_hash:\n",
    "        if len(line) > 1:\n",
    "            mean.append(np.mean(line[1:]))\n",
    "        else:\n",
    "            mean.append(np.mean(line))\n",
    "    return np.mean(mean)\n",
    "\n",
    "# Функция создания датасета с заполнением пропущенных значений числами из нормального распределения\n",
    "# На вход подаётся файл со словарём, возвращает функция датафрейм\n",
    "def Data(file):\n",
    "    dict_hash = Export_file(file)\n",
    "    df = pd.DataFrame(columns=np.arange(255*41))\n",
    "    for i in dict_hash.keys():\n",
    "        res = []\n",
    "        for line in dict_hash[i]:\n",
    "            # Строка заполняется числами из нормального распределения, со средним по строке числом, равным среднему числу\n",
    "            # в строке словаря файла и ср-кв отклонением - квадрат среднего значения\n",
    "            string = abs(np.random.normal(scale = Mean_list(dict_hash[i]), loc = Mean_list(dict_hash[i])**2, size = 255))\n",
    "            for j in range(len(line)):\n",
    "                string[j] = line[j] # заполняем числами из словаря\n",
    "            res.extend(string)\n",
    "        if len(dict_hash[i]) < 41: # проверка на кол-во строк, чтобы добавить пустые строки\n",
    "            res.extend(abs(np.random.normal(scale = np.mean(res), loc = np.mean(res)**2, size = (41 - len(dict_hash[i])) * 255 )))\n",
    "        s = pd.Series(res, name = i)\n",
    "        df = df.append(s)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание датафреймов\n",
    "df_res = Data('result_dictionary.txt')\n",
    "df_res = Data('result_dictionary.txt')\n",
    "df_sa = Data('SA_dict.txt')\n",
    "df_safe = Data('SAFE_dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переименовываем столбец\n",
    "df_res.rename(columns={'Unnamed: 0': 'Hash'}, inplace=True)\n",
    "df_sa.rename(columns={'Unnamed: 0': 'Hash'}, inplace=True)\n",
    "df_safe.rename(columns={'Unnamed: 0': 'Hash'}, inplace=True)\n",
    "# Дописываем категории\n",
    "df_res['Category'] = [True for i in range(len(df_res))]\n",
    "df_sa['Category'] = [False for i in range(len(df_sa))]\n",
    "df_safe['Category'] = [True for i in range(len(df_safe))]\n",
    "# Объединяем датафреймы\n",
    "data = pd.concat([df_res, df_sa, df_safe])\n",
    "# Записываем датасет в файл\n",
    "Import_file(data, 'Data_Norm.csv')\n",
    "# Импортируем датасет\n",
    "data = Export_file('Data_Norm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция аналогичная функции для заполнения нормальным распределением, но заполняет пропущенные значения числами из\n",
    "# Пуассоновского потока событий\n",
    "def Data_Puasson(file):\n",
    "    dict_hash = Export_file(file)\n",
    "    df = pd.DataFrame(columns=np.arange(255*41))\n",
    "    for i in dict_hash.keys():\n",
    "        res = []\n",
    "        for line in dict_hash[i]:\n",
    "            string = np.random.sample(255)\n",
    "            for j in range(len(line)):\n",
    "                string[j] = line[j]\n",
    "            res.extend(string)\n",
    "        if len(dict_hash[i]) < 41:\n",
    "            res.extend(np.random.sample((41 - len(dict_hash[i])) * 255))\n",
    "        s = pd.Series(res, name = i)\n",
    "        df = df.append(s)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Действия аналогичные предыдущему заполнению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = Data_Puasson('result_dictionary.txt')\n",
    "df_sa = Data_Puasson('SA_dict.txt')\n",
    "df_safe = Data_Puasson('SAFE_dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.rename(columns={'Unnamed: 0': 'Hash'}, inplace=True)\n",
    "df_sa.rename(columns={'Unnamed: 0': 'Hash'}, inplace=True)\n",
    "df_safe.rename(columns={'Unnamed: 0': 'Hash'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res['Category'] = [True for i in range(len(df_res))]\n",
    "df_sa['Category'] = [False for i in range(len(df_sa))]\n",
    "df_safe['Category'] = [True for i in range(len(df_safe))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([df_res, df_sa, df_safe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Import_file(data, 'Data_Puasson.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Export_file('Data_Puasson.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, mean_squared_error, r2_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбиваем данные на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[data.columns[1:-1]], data['Category'], test_size=0.1, random_state=69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Наивный байес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6268099282339787\n",
      "0.6075949367088608\n",
      "0.5936849649946884\n",
      "0.6075949367088608\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "predict = model.predict(X_test)\n",
    "core = f1_score(y_test, predict, average='weighted')\n",
    "score = accuracy_score(y_test, predict)\n",
    "print(core)\n",
    "print(score)\n",
    "print(precision_score(predict, y_test, average='weighted'))\n",
    "print(recall_score(predict, y_test, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.35      0.53      0.43        86\n",
      "        True       0.78      0.63      0.70       230\n",
      "\n",
      "    accuracy                           0.61       316\n",
      "   macro avg       0.57      0.58      0.56       316\n",
      "weighted avg       0.67      0.61      0.63       316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7531645569620253\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.55      0.51      0.53        86\n",
      "        True       0.82      0.84      0.83       230\n",
      "\n",
      "    accuracy                           0.75       316\n",
      "   macro avg       0.69      0.68      0.68       316\n",
      "weighted avg       0.75      0.75      0.75       316\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rtyde\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model3 = LogisticRegression().fit(X_train, y_train)\n",
    "predict3 = model3.predict(X_test)\n",
    "print(model3.score(X_test, y_test))\n",
    "print(classification_report(y_test, predict3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Случайные леса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 =  RandomForestRegressor()\n",
    "RandomFor = model1.fit(X_train, y_train)\n",
    "predict1 = model1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_1 = rounding(predict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8713728522782372\n",
      "0.870253164556962\n",
      "0.12974683544303797\n"
     ]
    }
   ],
   "source": [
    "core1 = f1_score(y_test, predict_1, average='weighted')\n",
    "score1 = accuracy_score(y_test, predict_1)\n",
    "print(core1)\n",
    "print(score1)\n",
    "print(mean_squared_error(predict_1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.75      0.79      0.77        86\n",
      "        True       0.92      0.90      0.91       230\n",
      "\n",
      "    accuracy                           0.87       316\n",
      "   macro avg       0.83      0.85      0.84       316\n",
      "weighted avg       0.87      0.87      0.87       316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predict_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лассо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LassoCV(normalize=True, tol=0.01)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "model = LassoCV(normalize = True, tol = 0.01)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7765048413072123\n",
      "0.7753164556962026\n",
      "0.22468354430379747\n",
      "-0.11052813938523975\n",
      "0.7753164556962026\n"
     ]
    }
   ],
   "source": [
    "prediction  = model.predict(X_test)\n",
    "predict_L = rounding(prediction)\n",
    "print(f1_score(y_test, predict_L, average='weighted'))\n",
    "print(accuracy_score(y_test, predict_L))\n",
    "print(mean_squared_error(predict_L, y_test))\n",
    "print(r2_score(predict_L, y_test))\n",
    "print(np.mean(predict_L == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.58      0.60      0.59        86\n",
      "        True       0.85      0.84      0.84       230\n",
      "\n",
      "    accuracy                           0.78       316\n",
      "   macro avg       0.72      0.72      0.72       316\n",
      "weighted avg       0.78      0.78      0.78       316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predict_L))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
